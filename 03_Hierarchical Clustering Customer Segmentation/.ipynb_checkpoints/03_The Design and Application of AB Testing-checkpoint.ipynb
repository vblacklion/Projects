{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Design and Application of A/B Testing\n",
    "0%\n",
    "In this chapter you will dive fully into A/B testing. You will learn the mathematics and knowledge needed to design and successfully plan an A/B test from determining an experimental unit to finding how large a sample size is needed. Accompanying this will be an introduction to the functions and code needed to calculate the various quantities associated with a statistical test of this type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B test generalizability Listed below are a set of decisions that could be made when designing an A/B test. Identify the decision that would not cause an issue in generalizing the test results to the overall user population.\n",
    "\n",
    "Answer the question 50 XP Possible Answers \n",
    "\n",
    "Assigning users to the Test or Variant group based on their signup year. press 1 \n",
    "\n",
    "Using a hash of the randomly assigned user id to determine user groupings. (CORRECT ANSWER - This is a fine thing to do and a common way to tie the group a user belongs to to their identity.) press 2 \n",
    "\n",
    "Randomly assigning users within one country to different groups. press 3 \n",
    "\n",
    "Allowing users to change groups every time they use the service or software. press"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental units: Revenue per user day\n",
    "We are going to check what happens when we add a consumable paywall to our app. A paywall is a feature of a website or other technology that requires payment from users in order to access additional content or services.\n",
    "\n",
    "Here, you'll practice calculating experimental units and baseline values related to our consumable paywall. Both measure revenue only among users who viewed a paywall. Your job is to calculate revenue per user-day, with user-day as the experimental unit.\n",
    "\n",
    "The purchase_data dataset has been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Extract the 'day' value from the date timestamp as you saw in the video: Using .date.dt.floor('d').\n",
    "To make the calculations easier, replace the NaN purchase_data.price values with 0 by using the np.where() method.\n",
    "Finally, find the mean amount paid per user-day among paywall viewers. To do this, you need to first aggregate the data by 'uid' and 'date', which has been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'day'; value from the timestamp\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Replace the NaN price values with 0 \n",
    "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
    "\n",
    "# Aggregate the data by 'uid' & 'date'\n",
    "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
    "revenue_user_day = purchase_data_agg.sum()\n",
    "\n",
    "# Calculate the final average\n",
    "revenue_user_day = revenue_user_day.price.mean()\n",
    "print(revenue_user_day)\n",
    "\n",
    "# Values such as these will provide helpful context as you prepare your experiment. Now lets learn how to run an A/B test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion rate sensitivities\n",
    "To mix things up, we will spend the next few exercises working with the conversion rate metric we explored in Chapter One. Specifically you will work to examine what that value becomes under different percentage lifts and look at how many more conversions per day this change would result in. First you will find the average number of paywall views and purchases that were made per day in our observed sample. Good luck!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instructions\n",
    "100 XP\n",
    "Merge the paywall_views with demographics_data tables using an 'inner' join. This will limit the result to only include users who appear in both and will remove everyone who did not view a paywall, which is what we want in this scenario.\n",
    "Group purchase_data by 'date'. The result of this is then aggregated for you by summing over the purchase field to find the total number of purchases and counting over it to find the total number of paywall views.\n",
    "Average each of the resulting sum and count fields to find the average number of purchases and paywall views per day.\n",
    "The results reflect a sample of 0.1% of our overall population for ease of use. Multiply each of daily_purchases and daily_paywall_views by 1000 so our result reflects the magnitude change if we had been observing the entire population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and group the datasets\n",
    "purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Group and aggregate our combined dataset \n",
    "daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)\n",
    "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
    "\n",
    "# Find the mean of each field and then multiply by 1000 to scale the result\n",
    "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
    "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
    "daily_purchases = daily_purchases * 1000\n",
    "daily_paywall_views = daily_paywall_views * 1000\n",
    "\n",
    "print(daily_purchases)\n",
    "print(daily_paywall_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity\n",
    "Continuing with the conversion rate metric, you will now utilize the results from the previous exercise to evaluate a few potential sensitivities that we could make use of in planning our experiment. The baseline conversion_rate has been loaded for you, calculated in the same way we saw in Chapter One. Additionally the daily_paywall_views and daily_purchases values you calculated previously have been loaded.\n",
    "\n",
    "Instructions 1/3\n",
    "30 XP\n",
    "Instructions 1/3\n",
    "30 XP\n",
    "1\n",
    "Using the proposed small_sensitivity of 0.1, find the lift in conversion rate and purchasers that would result by applying this sensitivity. Are these resulting values reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sensitivity = 0.1 \n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "small_purchasers = daily_paywall_views * small_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = small_purchasers - daily_purchases\n",
    "\n",
    "print(small_conversion_rate)\n",
    "print(small_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "Now repeating the steps from before, find the lift in conversion rate and purchasers using the medium_sensitivity. In this exercise you are additionally asked to complete the step to find the increase in purchasers based on this new conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sensitivity = 0.2\n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = medium_purchasers - daily_purchases\n",
    "\n",
    "print(medium_conversion_rate)\n",
    "print(medium_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\n",
    "Finally repeat the steps from before to find the increase in conversion rate and purchasers when using the very large sensitivity of 0.5. The steps required are the same as the previous exercise. How do the results compare those returned in the previous two exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sensitivity = 0.5\n",
    "\n",
    "# Find the conversion rate lift with the sensitivity above\n",
    "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
    "\n",
    "# Find how many more users per day that translates to\n",
    "large_purchasers = daily_paywall_views * large_conversion_rate\n",
    "purchaser_lift = large_purchasers - daily_purchases\n",
    "\n",
    "print(large_conversion_rate)\n",
    "print(large_purchasers)\n",
    "print(purchaser_lift)\n",
    "\n",
    "#  While it seems that a 50% increase may be too drastic and unreasonable to expect,\n",
    "# the small and medium sensitivities both seem very reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard error\n",
    "Previously we observed how to calculate the standard deviation using the .std() method. In this exercise, you will explore how to calculate standard deviation for a conversion rate, which requires a slightly different procedure. You will calculate this step by step in this exercise.\n",
    "\n",
    "Loaded for you is our inner merged dataset purchase_data as well as the computed conversion_rate value.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instructions\n",
    "100 XP\n",
    "Find the number of paywall views in the dataset using .count(). Store this in n.\n",
    "Calculate a quantity we will call v by finding the conversion_rate times the rate of not converting.\n",
    "Now find our variance, var, by dividing v by n. This is the variance of our conversion rate estimate.\n",
    "Finally the square root of var has been taken and stored as the variable se for you. This is the standard error of our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of paywall views \n",
    "n = purchase_data.purchase.count()\n",
    "\n",
    "# Calculate the quantitiy \"v\"\n",
    "v = conversion_rate * (1 - conversion_rate) \n",
    "\n",
    "# Calculate the variance and standard error of the estimate\n",
    "var = v / n \n",
    "se = var**0.5\n",
    "\n",
    "print(var)\n",
    "print(se)\n",
    "\n",
    "# Notice how closely the standard error is related to our sample size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the power calculation\n",
    "As discussed, power is the probability of rejecting the null hypothesis when the alternative hypothesis is true. Here you will explore some properties of the power function and see how it relates to sample size among other parameters. The get_power() function has been included and takes the following arguments in the listed order n for sample size, p1 as the baseline value, p2 as the value with lift included, and cl as the confidence level.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Instructions\n",
    "100 XP\n",
    "Calculate the power using n = 1000 and n = 2000 in that order, along with the pre-loaded parameters, p1, p2, and cl.\n",
    "Using the variable n1 for the sample size, find the power with a confidence level of cl = 0.8 and cl = 0.95 in that order.\n",
    "Hit 'Submit Answer' to compare the ratios. Which change has the bigger impact, increasing the confidence level or the sample size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the impact of sample size increase on power\n",
    "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
    "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
    "\n",
    "# Look at the impact of confidence level increase on power\n",
    "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
    "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
    "    \n",
    "# Compare the ratios\n",
    "print(n_param_two / n_param_one)\n",
    "print(alpha_param_one / alpha_param_two)\n",
    "\n",
    "# With these particular values it looks like decreasing our confidence level has a slightly larger impact on the power\n",
    "# than increasing our sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the sample size\n",
    "You're now going to utilize the sample size function to determine how many users you need for the test and control groups under various circumstances.\n",
    "\n",
    "Included is the get_sample_size() function you viewed previously, which takes four primary arguments, power, p1, p2 and cl as described before:\n",
    "\n",
    "def get_sample_size(power, p1, p2, cl, max_n=1000000):\n",
    "    n = 1 \n",
    "    while n <= max_n:\n",
    "        tmp_power = get_power(n, p1, p2, cl)\n",
    "\n",
    "        if tmp_power >= power: \n",
    "            return n \n",
    "        else: \n",
    "            n = n + 100\n",
    "\n",
    "    return \"Increase Max N Value\"\n",
    "\n",
    "You will continue working with the paywall conversion rate data for this exercise, which has been pre-loaded as purchase_data.\n",
    "\n",
    "Instructions 1/3\n",
    "\n",
    "Calculate the baseline conversion_rate per paywall view by dividing the total amount spent across all purchase_data.purchase values by the count of purchase_data.purchase values in the dataset.\n",
    "\n",
    "Instructions 2/3\n",
    "\n",
    "Great! Using the conversion_rate value you found, calculate p2, the baseline increased by the percent lift listed.\n",
    "\n",
    "Calculate the sample size needed using the parameters provided in the code comments. Remember the order of the arguments for get_sample_size is power, baseline conversion rate, lifted conversion rate and confidence level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "            \n",
    "# Desired Power: 0.8\n",
    "# CL: 0.90\n",
    "# Percent Lift: 0.1\n",
    "p2 = conversion_rate * (1 + 0.1)\n",
    "sample_size = get_sample_size(0.8, conversion_rate, p2, 0.90)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions 3/3\n",
    "\n",
    "Repeat the steps in the previous exercise only now with the new power parameter provided. How does increasing our desired power impact the outputed sample size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "\n",
    "# Desired Power: 0.95\n",
    "# CL 0.90\n",
    "# Percent Lift: 0.1\n",
    "p2 = conversion_rate * (1 + 0.1)\n",
    "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
    "print(sample_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
